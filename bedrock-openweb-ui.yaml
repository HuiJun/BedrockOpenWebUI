services:
  reverse-proxy:
    container_name: reverse-proxy
    image: docker.io/library/caddy:2-alpine
    restart: unless-stopped
    volumes:
      - ./assets/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
    networks:
      - open-webui-bridge
    ports:
      - 80:80
      - 443:443
    environment:
      - TARGET=http://open-webui:${OPENWEBUI_PORT:-8080}

  open-webui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:main
    pull_policy: always
    restart: unless-stopped
    volumes:
      - open-webui:/app/backend/data
    networks:
      - open-webui-bridge
    environment:
      - PORT=${OPENWEBUI_PORT:-8080}
      - HOST=0.0.0.0
      - OPENAI_API_BASE_URL=http://litellm-server:${LITELLM_PORT:-4000}
      - OPENAI_API_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      - ENABLE_OLLAMA_API=True
      - OLLAMA_BASE_URL=http://ollama-server:11434
      - DEFAULT_MODELS=${LOCAL_MODEL:-hf.co/lmstudio-community/Qwen3-4B-Instruct-2507-GGUF:Q8_0}
      - REDIS_URL=redis://valkey-server:6379/0
      - SEARXNG_QUERY_URL=http://searxng-server:${SEARXNG_PORT:-8888}/search?q=<query>
      - CONTENT_EXTRACTION_ENGINE=tika
      - TIKA_SERVER_URL=http://tika-server:9998
      - USE_CUDA_DOCKER=True
      - MODELS_CACHE_TTL=1210000
      - ENABLE_WEB_SEARCH=True
      - WEB_SEARCH_ENGINE=searxng
      - WEB_SEARCH_TRUST_ENV=True
      - WEB_SEARCH_RESULT_COUNT=3
      - WEB_SEARCH_CONCURRENT_REQUESTS=10
      - BYPASS_EMBEDDING_AND_RETRIEVAL=${BYPASS_EMBEDDING_AND_RETRIEVAL:-True}

  litellm-server:
    container_name: litellm-server
    image: ghcr.io/berriai/litellm:main-stable
    volumes:
      - ./assets/litellm/config.yaml:/app/config.yaml
    restart: unless-stopped
    command: ["--config", "/app/config.yaml"]
    networks:
      - open-webui-bridge
    environment:
      - PORT=${LITELLM_PORT:-4000}
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION:-us-west-2}
      - AZURE_CLIENT_ID=${AZURE_CLIENT_ID}
      - AZURE_TENANT_ID=${AZURE_TENANT_ID}
      - AZURE_CLIENT_SECRET=${AZURE_CLIENT_SECRET}
      - APIM_SUBSCRIPTION_KEY=${APIM_SUBSCRIPTION_KEY}
      - AZURE_API_BASE=${AZURE_API_BASE}
      - AZURE_API_VERSION=${AZURE_API_VERSION}

  ollama-server:
    container_name: ollama-server
    image: ollama/ollama:latest
    pull_policy: always
    entrypoint: [ "/bin/bash", "/entrypoint.sh" ]
    tty: true
    restart: unless-stopped
    volumes:
      - ollama:/root/.ollama
      - ./assets/ollama/entrypoint.sh:/entrypoint.sh
    networks:
      - open-webui-bridge
    environment:
      - OLLAMA_WEBAPI_PORT=${OLLAMA_WEBAPI_PORT:-11434}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - LOCAL_MODEL=${LOCAL_MODEL:-hf.co/lmstudio-community/Qwen3-4B-Instruct-2507-GGUF:Q8_0}

  valkey-server:
    container_name: valkey-server
    image: valkey/valkey:8-alpine
    command: valkey-server --save 30 1 --loglevel warning
    restart: unless-stopped
    volumes:
      - valkey:/data
    networks:
      - open-webui-bridge

  searxng-server:
    container_name: searxng-server
    image: ghcr.io/searxng/searxng:latest
    restart: unless-stopped
    networks:
      - open-webui-bridge
    volumes:
      - searxng:/var/cache/searxng
      - ./assets/searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://searxng-server:${SEARXNG_PORT:-8888}/
      - SEARXNG_SECRET=${SEARXNG_SECRET:-somesearxngsecret}
      - SEARXNG_LIMITER=False
      - SEARXNG_PUBLIC_INSTANCE=False
      - GRANIAN_PORT=${SEARXNG_PORT:-8888}

  tika-server:
    image: apache/tika:latest-full
    container_name: tika-server
    restart: unless-stopped
    networks:
      - open-webui-bridge

networks:
  open-webui-bridge:
    name: open-webui-bridge
    driver: bridge

volumes:
  ollama: {}
  open-webui: {}
  valkey: {}
  searxng: {}
